{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from script.model import Model\n",
    "from script.approximator import PCALocalApproximation, GNNLocalApproximation, SurfaceDerivative\n",
    "from script.auxiliary import *\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c588ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet_TimeIndependent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet_TimeIndependent, self).__init__()\n",
    "        self.fc0 = nn.Linear(3, 128)\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc0.weight)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        u = torch.sin(torch.pi*self.fc0(x))\n",
    "        u = torch.sin(torch.pi*self.fc1(u))\n",
    "        u = torch.sin(torch.pi*self.fc2(u))\n",
    "        u = self.fc3(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5777cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "\n",
    "num_pts = int(input('The number of Points : '))\n",
    "args.num_pts = num_pts ######################\n",
    "\n",
    "args.surface = 'gnn'\n",
    "args.gnn_dir = '../section_31/save/trained_GNN_model.pt' ###############\n",
    "args.K = int(input('K : ')) #####################\n",
    "args.ratio = 1.0\n",
    "args.t_batch_size = 10\n",
    "args.save_dir = 'OpenSurface_{}'.format(args.num_pts)\n",
    "args.target = 'OpenSurface_{}'.format(args.num_pts)\n",
    "args.gpu = 0 ###########################\n",
    "\n",
    "args.lr = 1e-3\n",
    "args.epochs = 20000\n",
    "sch_Step_Size = 2000\n",
    "sch_Gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_type = 'open_surface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sin, pi\n",
    "\n",
    "# def eval_z(x,y):\n",
    "#     return 0.5*(x*x-y*y)+3/20*sin(args.a*pi*x)*sin(args.b*pi*y)\n",
    "\n",
    "def eval_u(x,y,z,t):\n",
    "    return (0.75 - x) * sin(y - z)\n",
    "\n",
    "def eval_u_point(points):\n",
    "    x = points[:,0].clone()\n",
    "    y = points[:,1].clone()\n",
    "    z = points[:,2].clone()\n",
    "    return eval_u(x,y,z,0)\n",
    "\n",
    "def eval_phi(x,y,z):\n",
    "    return y**2 + z**2 - ((1-0.25*x**2)**0.5 * (0.75*x**2+0.5))**2\n",
    "\n",
    "#compute true value of Laplace_Beltrami Operator\n",
    "def laplace_beltrami_torch(eval_u_torch, eval_phi_torch, x,y,z,t):\n",
    "    u = eval_u_torch(x,y,z,t)\n",
    "    u_x = compute_grad(u,x)\n",
    "    u_y = compute_grad(u,y)\n",
    "    u_z = compute_grad(u,z)\n",
    "    \n",
    "    grad_u = torch.stack([u_x,u_y,u_z],1)\n",
    "    \n",
    "    u_xx = compute_grad(u_x,x)\n",
    "    u_xy = compute_grad(u_x,y)\n",
    "    u_xz = compute_grad(u_x,z)\n",
    "    \n",
    "    u_yx = compute_grad(u_y,x)\n",
    "    u_yy = compute_grad(u_y,y)\n",
    "    u_yz = compute_grad(u_y,z)\n",
    "    \n",
    "    u_zx = compute_grad(u_z,x)\n",
    "    u_zy = compute_grad(u_z,y)\n",
    "    u_zz = compute_grad(u_z,z)\n",
    "    \n",
    "    lapla_u = (u_xx+u_yy+u_zz).reshape(-1,1)\n",
    "    \n",
    "    hess_u = torch.stack([u_xx,u_xy,u_xz,u_yx,u_yy,u_yz,u_zx,u_zy,u_zz],1).reshape(-1,3,3)\n",
    "    \n",
    "    phi = eval_phi_torch(x,y,z)\n",
    "    \n",
    "    phi_x = compute_grad(phi,x)\n",
    "    phi_y = compute_grad(phi,y)\n",
    "    phi_z = compute_grad(phi,z)\n",
    "    \n",
    "    phi_xx = compute_grad(phi_x,x)\n",
    "    phi_xy = compute_grad(phi_x,y)\n",
    "    phi_xz = compute_grad(phi_x,z)\n",
    "    \n",
    "    phi_yx = compute_grad(phi_y,x)\n",
    "    phi_yy = compute_grad(phi_y,y)\n",
    "    phi_yz = compute_grad(phi_y,z)\n",
    "    \n",
    "    phi_zx = compute_grad(phi_z,x)\n",
    "    phi_zy = compute_grad(phi_z,y)\n",
    "    phi_zz = compute_grad(phi_z,z)\n",
    "    \n",
    "    grad_phi = torch.stack([phi_x,phi_y,phi_z],1)\n",
    "    norm_grad_phi = torch.sqrt(phi_x**2+phi_y**2+phi_z**2).reshape(-1,1)\n",
    "    \n",
    "    n = grad_phi / norm_grad_phi\n",
    "    \n",
    "    d_n_u = (grad_u*n).sum(1).reshape(-1,1)\n",
    "    \n",
    "    lapla_phi = (phi_xx+phi_yy+phi_zz).reshape(-1,1)\n",
    "    \n",
    "    hess_phi = torch.stack([\n",
    "        phi_xx,phi_xy,phi_xz,\n",
    "        phi_yx,phi_yy,phi_yz,\n",
    "        phi_zx,phi_zy,phi_zz\n",
    "    ],1).reshape(-1,3,3)\n",
    "    \n",
    "    nth_phin = bmm3(n.reshape(-1,1,3),hess_phi,n.reshape(-1,3,1)).reshape(-1,1)\n",
    "    twoH = (lapla_phi - nth_phin)/norm_grad_phi\n",
    "    nth_un = bmm3(n.reshape(-1,1,3),hess_u,n.reshape(-1,3,1)).reshape(-1,1)\n",
    "    return lapla_u - twoH*d_n_u - nth_un\n",
    "\n",
    "\n",
    "#get the values of f, where f = u_t - LaplaceBeltrami(u)\n",
    "def eval_f_torch(eval_u_torch, eval_phi_torch, x,y,z,t):\n",
    "    u = eval_u_torch(x,y,z,t)\n",
    "    u_t = compute_grad(u,t).reshape(-1,1)\n",
    "    return u_t - laplace_beltrami_torch(eval_u_torch,eval_phi_torch,x,y,z,t)\n",
    "\n",
    "\n",
    "#Redefine above functions to get 'points' as inputs directly  \n",
    "def laplace_beltrami_point(eval_u, eval_phi, points, t):\n",
    "    x = points[:,0].clone()\n",
    "    x.requires_grad = True\n",
    "    y = points[:,1].clone()\n",
    "    y.requires_grad = True\n",
    "    z = points[:,2].clone()\n",
    "    z.requires_grad = True\n",
    "    return laplace_beltrami_torch(eval_u, eval_phi, x,y,z, t)\n",
    "\n",
    "def get_loss_ge(model, batch, device):\n",
    "    x, weight, basis, A_inv, derivative = batch\n",
    "    x = x.to(device)\n",
    "    weight = weight.to(device)\n",
    "    basis = basis.to(device)\n",
    "    A_inv = A_inv.to(device)\n",
    "    derivative = derivative.to(device)\n",
    "    \n",
    "    \n",
    "    u = model(x).reshape(-1,args.K)\n",
    "    \n",
    "    lb = laplace_beltrami_point(eval_u, eval_phi, x.reshape(-1,args.K,3)[:,0],0)\n",
    "    \n",
    "    #print(f.mean())\n",
    "    \n",
    "    b = (weight.reshape(-1,args.K,1) * basis * u.reshape(-1,args.K,1)).sum(1)\n",
    "    coef_u = (A_inv * b.unsqueeze(1)).sum(-1)\n",
    "    laplacian = derivative.laplacian(coef_u).reshape(-1,1)\n",
    "\n",
    "    loss_ge = ((laplacian - lb)**2).mean()\n",
    "    \n",
    "    return loss_ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gpu == -1:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(args.gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.load('../../dataset/Section_42/Position_Vectors/OpenSurface_' + str(num_pts) + '_Position.npy')).float()\n",
    "\n",
    "# min_distance = lambda cloud: min(pdist(cloud))\n",
    "# md = min_distance(X_both)\n",
    "# index_bd = (abs(X_both[:,0]-0.5)<0.5*md) + (abs(X_both[:,0]+0.5)<0.5*md) + (abs(X_both[:,1]-0.5)<0.5*md) + (abs(X_both[:,1]+0.5)<0.5*md)\n",
    "\n",
    "from torch import sin, pi, cos\n",
    "theta = torch.linspace(0,2*pi,100)\n",
    "radius = (1-0.25*0.75**2)**0.5 * (0.75*0.75**2+0.5)\n",
    "cos_theta = radius*cos(theta)\n",
    "sin_theta = radius*sin(theta)\n",
    "X_bd = torch.stack((torch.ones_like(theta)*0.75, cos_theta, sin_theta), 1)\n",
    "\n",
    "u_true = eval_u_point(X)\n",
    "u_true_bd = eval_u_point(X_bd).to(device)\n",
    "\n",
    "if args.surface == 'pca':\n",
    "    surface = PCALocalApproximation(args, torch.cat((X,X_bd),0))\n",
    "elif args.surface == 'gnn':\n",
    "    gnn_model = Model().to(device)\n",
    "    _, _, state_dict = torch.load(args.gnn_dir)\n",
    "    gnn_model.load_state_dict(state_dict)\n",
    "    surface = GNNLocalApproximation(args, torch.cat((X,X_bd),0), gnn_model)\n",
    "\n",
    "x = surface.X_knn[:len(X)]\n",
    "weight = surface.weight[:len(X)]\n",
    "basis = surface.basis[:len(X)]\n",
    "coef_a = surface.coef_a[:len(X)]\n",
    "tangent_vectors = surface.tangent_vectors[:len(X)]\n",
    "normal_vectors = surface.normal_vectors[:len(X)]\n",
    "\n",
    "batch_size = len(X)//3\n",
    "t_N = args.t_batch_size\n",
    "batches = []\n",
    "for i in range(0,len(x),batch_size):\n",
    "    x_batch = x[i:i+batch_size].repeat(t_N,1,1).reshape(-1,3)\n",
    "    weight_batch = weight[i:i+batch_size].repeat(t_N,1)\n",
    "    basis_batch = basis[i:i+batch_size].repeat(t_N,1,1)\n",
    "\n",
    "    A_batch = (weight_batch.reshape(-1,args.K,1,1) * basis_batch.reshape(-1,args.K,6,1) * basis_batch.reshape(-1,args.K,1,6)).sum(1)\n",
    "    A_inv_batch = torch.linalg.inv(A_batch)\n",
    "\n",
    "    derivative_batch = SurfaceDerivative(coef_a[i:i+batch_size].repeat(t_N,1), \n",
    "                                         tangent_vectors[i:i+batch_size].repeat(t_N,1,1), \n",
    "                                         normal_vectors[i:i+batch_size].repeat(t_N,1))\n",
    "\n",
    "    batches.append((x_batch, \n",
    "                    weight_batch, \n",
    "                    basis_batch,\n",
    "                    A_inv_batch, \n",
    "                    derivative_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e376472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet_TimeIndependent().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=sch_Step_Size, gamma=sch_Gamma)\n",
    "\n",
    "logs = dict()\n",
    "logs['loss_ge'] = []\n",
    "logs['loss_bd'] = []\n",
    "logs['l2_error'] = []\n",
    "logs['max_error'] = []\n",
    "\n",
    "# print(u_true.mean(), u_true.max(), u_true.min())\n",
    "\n",
    "epochs = args.epochs\n",
    "for epoch in tqdm(range(1,epochs+1)):\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    batch_loss_ge = 0.\n",
    "    batch_loss_bd = 0.\n",
    "    for batch in batches:\n",
    "        optimizer.zero_grad()\n",
    "        loss_ge = get_loss_ge(model, batch, device)\n",
    "\n",
    "\n",
    "        u_bd = model(X_bd.to(device)).squeeze()\n",
    "\n",
    "        loss_bd = ((u_true_bd - u_bd)**2).mean()\n",
    "\n",
    "        loss = loss_ge + loss_bd\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss_ge += loss_ge.item()\n",
    "        batch_loss_bd += loss_bd.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    logs['loss_ge'].append(batch_loss_ge / len(batches))\n",
    "    logs['loss_bd'].append(batch_loss_bd / len(batches))\n",
    "\n",
    "    if epoch%100==0:\n",
    "        model.eval()\n",
    "        x_test = X.to(device)\n",
    "        u_pred = model(x_test).detach().cpu().reshape(-1)\n",
    "        print('\\n', epoch)\n",
    "        logs['l2_error'].append(rel_l2_error(u_true, u_pred).item())\n",
    "        logs['max_error'].append(rel_max_error(u_true, u_pred).item())\n",
    "\n",
    "        \"\"\"\n",
    "        argmx = (u_pred-u_true).argmax()\n",
    "\n",
    "        print(u_pred.shape, u_true.shape)\n",
    "        print(argmx)\n",
    "\n",
    "        print(u_pred[argmx])\n",
    "        \"\"\"\n",
    "        #print('loss_ge',logs['loss_ge'][-1], 'loss_bd',logs['loss_bd'][-1])\n",
    "        print('l2_error',logs['l2_error'][-1],'max_error',logs['max_error'][-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b07bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save((logs, model.cpu().state_dict()), 'save/' + save_dir)\n",
    "save_path = './save/Section_42/OpenSurface_{}.pt'.format(num_pts)\n",
    "torch.save((logs, model.cpu().state_dict()), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(logs['loss_ge'], label=r'$Loss_{GE}$')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(logs['l2_error'], label=r'$L_2 error$')\n",
    "plt.plot(logs['max_error'], label=r'$L_\\infty error$')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
